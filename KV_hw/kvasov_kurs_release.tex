\documentclass[12pt,fleqn]{article}

\usepackage[cp1251]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amssymb,amsmath,mathrsfs,amsthm}
\usepackage[russian]{babel}
\usepackage{graphicx}
\usepackage[footnotesize]{caption2}
\usepackage{indentfirst}
%\usepackage[ruled,section]{algorithm}
%\usepackage[noend]{algorithmic}
%\usepackage[all]{xy}

% Параметры страницы
\textheight=24cm
\textwidth=16cm
\oddsidemargin=5mm
\evensidemargin=-5mm
\marginparwidth=36pt
\topmargin=-1cm
\footnotesep=3ex
%\flushbottom
\raggedbottom
\tolerance 3000
% подавить эффект "висячих стpок"
\clubpenalty=10000
\widowpenalty=10000
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\baselinestretch}{1.5} %для печати с большим интервалом

\usepackage{tabularx}
\usepackage[normal]{subfigure}
\usepackage{xfrac}
\newcommand\pN{\mathcal{N}}
\newcommand\Bold[1]{\mathbf #1}
\newcommand{\MExp}{\mathsf{M}}
\newcommand\Seq[1]{1,\dots,#1}
\newcommand\dif{\mathrm d}
\newcommand\setR{\mathbb R}
\newcommand\apos{\textquoteright}
\newcommand\wtilde[1]{\widetilde #1}
\newcommand\intcl{\intercal}
\DeclareMathOperator{\sign}{sign}

\begin{document}

\begin{titlepage}
\begin{center}
    Московский государственный университет имени М. В. Ломоносова

    \bigskip
    \includegraphics[width=50mm]{msu.eps}

    \bigskip
    Факультет Вычислительной Математики и Кибернетики\\
    Кафедра Математических Методов Прогнозирования\\[10mm]

    \textsf{\large\bfseries
        КУРСОВАЯ РАБОТА СТУДЕНТА 317 ГРУППЫ\\[10mm]
        Байесовский подход к обучению распознаванию образов с учетом критерия гладкости решающего правила \\ на основе метода опорных векторов
    }\\[10mm]

    \begin{flushright}
        \parbox{0.5\textwidth}{
            Выполнил:\\
            студент 3 курса 317 группы\\
            \emph{Квасов Андрей Федорович}\\[5mm]
            Научный руководитель:\\
            к.ф-м.н.\\
            \emph{Красоткина Ольга Вячеславовна}
        }
    \end{flushright}

    \vspace{\fill}
    Москва, 2015
\end{center}
\end{titlepage}

\newpage
\renewcommand{\contentsname}{Содержание}
\tableofcontents

\newpage
\begin{abstract}
    Обращаясь к конкретным прикладным задачам распознавания образов, часто объектами, данными, являются упорядоченные последовательности величин, которые характеризуют явления, свойства объектов и их изменение по какой-либо оси аргумента, в частности, по временной. Такого роды задачи, задачи распознавания сигналов отражены в более прикладных областях, к примеру, в распознавании рукописных символов. Из-за своей специфики возникают различные проблемы связанные с признаковым описанием объектов распознавания.
    
    В данной работе рассматривается метод опорных векторов, с использованием мягких отступов и ядер, применяемый в конкретной задаче распознавания рукописных символов, а также рассматривается вероятностная интерпретация наложенных на модель ограничений, связанных со спецификой задач распознавания символов.
\end{abstract}

\newpage
\section{Введение}
\label{sec:intro}
\subsection{Основные понятия и определения}
Задача распознавания сигналов, как часть задачи распознавания образов, оперирует измерениями каких-либо реальных объектов или явлений, расположенных упорядоченно по аргументу $t \in T = 1,\Seq N, \dots$, и представляющие из себя наборы векторов $x_t = x^1_t, \dots, x^n_t$, каждая координата которой может принадлежать множеству $\mathbb X^n$, не совпадающему для разных координат.

Помимо различия в значениях данных наблюдается одна из важных проблем распознавания сигналов: к примеру, рассмотрев написание букв от руки автором (\textit{online  handwriting}), и с какой-либо частотой регистрировать такие признаки как координата $(x, y)$ в ограничивающем символ окне, угол под которым находиться электронное перо или ручка, сила надавливания на экран регистрирующего устройства, скорость движения точек и т.п., для каждой буквы будет создано не фиксированное по длине признаковое описание. Таким образом, будет затруднительно сформировать пространство, удовлетворяющее гипотезе компактности, как основной гипотезе построения моделей и алгоритмов машинного обучения. Также нельзя использовать в качестве признаков отсчеты сигнала, взятые с некоторым шагом вдоль оси аргумента, поскольку сигналы, полученные от разных написаний даже одного и того же символа неизбежно будут иметь разную длину, и, следовательно. Значит не будет существовать единого линейного пространства, в котором могли бы быть представлены написания распознаваемых символов.

Фиксированность размеров признакового пространства в задаче машинного обучения является необходимым условием для не метрических алгоритмов таких как \textit{метод опорных векторов (Support Vector Machine)}, рассматриваемый в данной работе.

Вторая проблема, которая и была рассмотрена в данной работе, обозначает ситуацию, в которой исходная обучающая выборка мала и может лишь сравниться с размером признакового пространства. Для задачи распознавания рукописных символов свойственно, что количество объектов в обучающей совокупности почти равно среднему размеру признакового пространства, или много меньше (около 100 отсчетов на символ: букву, цифру и т.п.). Поэтому для получения большей точности на контрольной выборке, имея малоинформативную обучающую выборку, необходимо наложить априорные ограничения на вид признакового пространства. Одним из таких ограничений является \textit{критерий гладкости} для компонент в описании сигнала.

Как будет показано ниже, критерий гладкости представляет собой вид перехода пространства признаков из исходного пространства, в котором классификация имеет недостаточно хорошие результаты, в так называемое \textit{спрямляющее пространство}, определяющее новое, линейно разделимое множество точек выборки с новым признаковым описанием.

Данная концепция используется повсеместно, в особенности, в задачах распознаваня сигналов.

\subsection{Обзор литературы}

Для решения указанной ранее проблемы различного количества измерений объектов, когда признаковое описание реальных объектов, даже принадлежащих одному и тому же классу, является переменной величиной. В работе К. Бальмана \cite{Bahlman} представлен  \textit{Gaussian Dynamic Time Warping (GDTW)} ядровой переход. При котором, степень близости между объектами, необходимая в RBF-ядре, меняется с евклидового расстояния на метрику $D(\tau_i, \tau_j)$ другого вида. В свою очередь, данная метрика для объектов с разным количеством признаков выполняет процесс выравнивания, т.е. удлинения меньшего вектора признаков объекта, и далее рассматривает евклидовое расстояние между парами признаков. Этот процесс также называемый как Алгоритм динамической трансформации временной шкалы (\textit{\textrm{DTW}}) \cite{DTW}, применяется для перевода пары объектов в новое метрическое пространство. Похожая концепция применима и для критерия гладкости \cite{Krasotkina}.
Концепция \textrm{DTW} может быть применена в любых метрических классификаторах, таких как метод ближайших соседей \textrm{NN} или многослойный перцептрон Розенблатта \textrm{MLP} \cite{MLP}. Она не только дает возможность работы с сигналами с разной длинной признакового описания, но и повышает устойчивость алгоритма к шумам, позволяет использовать меру близости не только как метрику, но и новое признаковое описание объекта.

Во многих работах наряду с \textrm{SVM} в различных комбинациях применяются алгоритмы предобработки сигналов, генерации признаков, на основе которых и проводиться дальнейшая классификация. Состоящий из двух стадий алгоритм \textrm{HMM-SVM} \cite{HMM} применяет скрытую марковскую модель, а точнее совокупность скрытых марковских моделей для каждого отдельного класса, чтобы извлечь признаки из исходных "глобальных" признаков этого класса. Исходный вектор признаков может иметь различную длину для различных сигналов. Если так, то конечный вектор признаков определяется лишь тем, какие признаки были сгенерированны наборами HMM и тогда, помимо извлечения новых более информативных признаков, дающих вероятностную характеристику каждого класса и принадлежности данного объекта к классу, можно добиться эффекта уменьшения размерности признакового пространства. Иначе, мы добавляем извлеченные признаки к вектору глобальных и подаем на вход \textrm{SVM} с RBF ядром.

В данной работе будет рассмотрен вероятностный подход к распознаванию образов с помощью метода \textit{\textrm{Soft-margin SVM} с критерием гладкости }решающего правила.
 
\section{Вероятностная постановка задачи}
\subsection{Байесовское правило обучения}
Рассмотрим вероятностное пространство вида $X \times Y \times W$, которое соответствует декартовому произведению множества объектов на множество классов принадлежности объектов умноженное на множество параметров вероятностной модели распределения объектов и их классов.
\par
Обозначим $X^l = \left(\Bold x_i;y_i\right)_{i = 1}^{N},\, \Bold x_i \in X=\mathbb R^d,\, y_i \in Y =  \{-1, 1\}$, где  $N$ - количество объектов, $d$ - количество признаков объекта. $X^{l}$ - есть генеральная совокупность реализаций объектов $x_i$ и соответствующих ответов $y_i$. Пусть $\Bold w = \begin{bmatrix} \Bold a, & b \end{bmatrix}^\intercal$ - элемент множества $W = \setR^{d+1}$, где $\Bold a \in \setR^d, \, b \in \mathbb R $.
\par
Исходя из предложенного метода вероятностной интерпретации генеральной совокупности при решении задачи SVM \cite{tatar09bayes}, рассмотрим отнесение объекта выборки к классу, как разделение объектов в пространстве $\setR^d$ гиперплоскостью $\Bold a^\intercal \Bold x + b = 0$, соответствующей границе двух параметрических семейств - положительного класса $y_i = 1$ и отрицательного класса $y_i = -1$. Далее, будем учитывая гипотезу компактности распределения классов выборки, определим параметрическую плотность несобственного совместного распределения  объектов генеральной совокупности $\Bold x_i$ и его класса $y_i$ равной:
\begin{equation}
\label{eq:denseXY}
\begin{split}
	\varphi(\Bold x_i, y_i|\Bold w) &= \varphi(\Bold x_i, y_i|\Bold a, b) = \\
	&= \varphi_{y_i}(\Bold x_i|\Bold a, b) = \left\{
	\begin{aligned}
		1,&\,
		y_i(\Bold a^\intercal \Bold x_i + b) \geq 1; \\
		\exp[-\lambda(1 - y_i(\Bold a^\intercal \Bold x_i + b))],&\,
		y_i(\Bold a^\intercal \Bold x_i + b) < 1.
	\end{aligned}
	\right.
\end{split}
\end{equation}
\par
Несобственные распределения имеют особенность - интеграл от плотности распределения по всему пространству, на котором задано распределение, не равен единице,а бесконечен. Такая размытость распределения показывает свойство неопределенности распределения объектов в классе, находящихся за разделяющей полосой  внутри своего класса, т.е. могут свободно находиться в любом месте в пределах области своего класса \quad $\{x_i: y_i(\Bold a^\intercal \Bold x_i + b) \geq 1\}$. В то же время, вероятность найти объект класса внутри разделяющей полосы ($\{x_i: |\Bold a^\intercal \Bold x_i + b| \le 1\}$) или в полосе чужого класса ($\{x_i: y_i(\Bold a^\intercal \Bold x_i + b) < -1\}$) экспоненциально снижается к нулю, при удалении от границы своего класса разделяющей полосы.
\par
Под совместной плотностью распределения конечного множества векторов признаков объектов известных классов $(\Bold x_i, y_i)$ в составе обучающей совокупности $X^l$, полученных при наблюдении, будем понимать плотность распределения выборки независимых реализаций этих двух распределений:
\begin{align}
\Phi(X^l| \Bold a, b) &= \Phi(\Bold x_i, y_i, i=\Seq{N}| \Bold a, b) =\\
					 &= \prod_{i = 1}^N \varphi_{y_i}(\Bold x_i|\Bold a, b) = \left( \prod_{i:\ y_i=1} \varphi_1(\Bold x_i|\Bold a, b)\right) \left(\prod_{i:\ y_i= -1} \varphi_{-1}(\Bold x_i|\Bold a, b) \right).
\end{align}
\par
Вектор параметров $\Bold w$ задается априорным распределением вероятности на $W$ для параметров распределений $\varphi_y(\Bold x|\Bold a, b),\, y \in \{-1, 1\} $. Часть вектора параметров $\Bold a$ задается многомерным нормальным распределением  с нулевым вектором средних значений и матрицей ковариации вида $\sigma^2 \cdot B^{-1}$, т.е. положительно определенной и симметричной матрицей. Соответствующее $b$ распределение является несобственным равномерным распределением равным единице по всей прямой $\setR$.
Совместное несобственное априорное распределение в пространстве параметров модели генеральной совокупности имеет вид:
\begin{equation}
\label{eq:denseW}
\psi(\Bold w) = \psi(\Bold a, b) = \psi(\Bold a, b| \sigma^2, B) \propto \exp(-\frac{1}{2\sigma^2} \cdot \Bold a^\intercal B \Bold a).
\end{equation}
Видно, что совместное распределение $\psi(\Bold a, b)$ является также несобственным, так как не имеет конечного интеграла на множестве всех $b \in \setR$.
\par
Если по генеральной совокупности $X^l$ проводить восстановление вектора параметров $\Bold w$, исходя из разделения объектов по классам, то мы приходим к задаче нахождения максимума апостериорной плотности распределения параметров $\Bold a$ и
$b$ относительно обучающей совокупности, что соответствует задачи максимума правдоподобия (\textrm{ML}). Для ее определения воспользуемся формулой Байеса:
\begin{equation}
\begin{split}
	p(\Bold a, b| X^l) = p(\Bold a, b| x_i, y_i, i=\Seq N)
					  = \frac{\psi(\Bold a, b)\Phi(X^l| \Bold a, b)}{\idotsint\limits_{ \setR^{d+1}} \!\psi(\Bold a\apos, b\apos)\Phi(X^l| \Bold a\apos, b\apos)\, \dif \Bold a\apos \dif b\apos}.
\end{split}
\end{equation} 
Знаменатель справа не зависит от вектора параметров, таким образом:
\begin{equation}
\label{eq:Bayes}
p(\Bold a, b| X^l) \propto \psi(\Bold a, b)\Phi(X^l| \Bold a, b) = \psi(\Bold a, b)\left( \prod_{i:\ y_i=1} \varphi_1(\Bold x_i|\Bold a, b)\right) \left(\prod_{i:\ y_i= -1} \varphi_{-1}(\Bold x_i|\Bold a, b) \right).
\end{equation}
Принцип максимизации плотности апостериорного распределения в пространстве параметров модели генеральной совокупности приводит к байесовскому правилу обучения, взяв логарифм плотности \eqref{eq:Bayes}:
\begin{equation}
\label{eq:lnQ}
\begin{split}
	\ln p(\Bold a, b| X^l) &= \ln \psi(\Bold a, b) + \sum\limits_{i: y_i=1} \ln \varphi_1(\Bold x_i|\Bold a, b) + \sum\limits_{i: y_i=-1} \ln \varphi_{-1}(\Bold x_i|\Bold a, b) = \\
		&= \ln \psi(\Bold a, b) + \sum\limits_{i=1}^N \ln \varphi_{y_i}(\Bold x_i|\Bold a, b) \rightarrow \max_{\Bold a, b}
\end{split}
\end{equation}
\par
Для несобственных плотностей распределения векторов признаков
объектов двух классов \eqref{eq:denseXY} и априорного несобственного распределения
параметров разделяющей гиперплоскости \eqref{eq:denseW} байесовский критерий обучения \eqref{eq:lnQ} примет вид:
\begin{equation}
\label{eq:-lnQ}
\begin{split}
	Q = -\ln p(\Bold a, b| X^l) &= \frac{1}{2\sigma^2} \cdot \Bold a^\intercal B \Bold a +
		\sum\limits_{i=1}^N \lambda(1 - y_i(\Bold a^\intercal \Bold x_i + b)) \left[y_i(\Bold a^\intercal \Bold x_i + b) < 1 \right] = \\
		& = \frac{1}{2\sigma^2 \cdot \lambda} \cdot \Bold a^\intercal B \Bold a +
		\sum\limits_{i=1}^N max(0, 1 - y_i(\Bold a^\intercal \Bold x_i + b)) \rightarrow \min_{\Bold a, b}
\end{split}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%
\subsection{Soft-margin SVM и вероятностный подход}
\label{ssec:SMSVM}
\par
В формуле \eqref{eq:-lnQ} обозначим  $\xi_i = max(0,  1 - y_i(\Bold a^\intercal \Bold x_i + b))$. Получим SVM в виде задачи оптимизации, с ограничениями типа неравенства называемой \textrm{\bf Soft-margin SVM}:
\begin{equation}
\label{eq:SMSVM}
\left\{
		\begin{split}
		&\frac{1}{2C}\Bold a^\intercal B \Bold a +
		\sum\limits_{i=1}^N \xi_i \rightarrow \min_{\Bold a, \Bold \xi}\\	
		&y_i(\Bold a^\intercal \Bold x_i + b) \geq 1 - \xi_i,\, i=\Seq N\\
		&\xi_i \geq 0
		,\,i=\Seq N
		\end{split}
\right.
\end{equation}
\par
Такой подход к задаче классификации отражен в работе В. Вапника \cite{VapnikSVM}. Он предполагает рассматривать пространство признаков объекта обучающей выборки, в котором они не могут быть разделены гиперплоскостью на два подпространства, и в каждом объекты только одного класса. То есть ищется гиперплоскость, которая бы минимизировала ошибку разделения классов на ней, но не обязательно приравнивала ее к нулю. Ошибками, в данных нами обозначениях, являются неотрицательные величины $\xi_i = max(0,  1 - y_i(\Bold a^\intercal \Bold x_i + b))$. Ошибка $\xi_i$ будет нулевой, если будет верно неравенство $1 - y_i(\Bold a^\intercal \Bold x_i + b) \leq 0 \Leftrightarrow y_i(\Bold a^\intercal \Bold x_i + b) \geq 1$, т.е. отступ любого объекта соответствующего класса будет минимальным и выходящим за разделяющую полосу. соответственно отступом (\textrm{margin}) называют $M(x_i, y_i) = y_i(\Bold a^\intercal \Bold x_i + b)$, который равен проекции вектора признаков на нормаль к разделяющей гиперплоскости, т.е. расстояние от точки x до нее. Рассматривая задающуюся нормалью $\Bold a$ и сдвигом $b$ искомую разделяющую гиперплоскость $\Bold a^\intercal \Bold x_i + b = 0$, и ответом классификатора для нового объекта будет являться $a(\Bold x) = \sign \left(\Bold a^\intercal \Bold x + b \right)$.
\par 

Минимизируемый функционал качества в классической задаче \textrm{Soft-margin SVM} не включает матрицу $B$ (см. пункт \ref{ssec:kernel}). Помимо минимизации ошибки разделения объектов гиперплоскостью идея \textrm{SVM} состоит в том, чтобы максимизировать расстояние от ближайших объектов выборки до разделяющей гиперплоскости. В работе Вапника показано, что ширина разделяющей полосы, определяемая расстоянием между объектами двух классов с отступами равными $1$ и $-1$, равна $\displaystyle\dfrac{2}{\lVert\Bold w\rVert}$. \qquad  \qquad Для удобства решения задачи оптимизации ищется максимум $\gamma(\Bold w) =\frac{\lVert\Bold w\rVert^2}{2}$. Таким образом функционал полностью состоит из суммы двух слагаемых, первый из которых - это функционал потери $L(X^l, \Bold w) = \sum_{i=1}^N \xi_i$, а второй - регуляризатор $\gamma(\Bold w)$, т.е. ограничения накладываемые на значения искомого вектора параметров модели. 
\par
Проводя аналогию с вероятностным подходом, видно, что формула распределения объектов обучающей совокупности $\eqref{eq:denseXY}$ соответствует функции потерь $L(X^l, \Bold w)$ на этой совокупности, а априорное распределение параметров модели $\psi(\Bold w)$ \eqref{eq:denseW} - регуляризатору $\gamma(\Bold w)$.
\par
В отличие от классической постановки, при учете критерия гладкости регуляризатор имеет другой вид $\gamma(\Bold w) = \frac{1}{2C}\Bold a^\intercal B \Bold a$. Рассмотрим этот вид задачи позднее (см. пункт \ref{ssec:kernel}).
Как уже было сказано, этап обучения состоит из минимизации функционала, состоящего из двух слагаемых:
\begin{equation}
\label{eq:Q}
\begin{split}
&Q(X^l,\Bold w) = \gamma(\Bold w) + C \cdot L(X^l, \Bold w) \rightarrow \min_{\Bold w}\\
&\gamma(\Bold w) = \frac{1}{2C}\Bold a^\intercal B \Bold a = -\ln\psi(\Bold w);\\
&L(X^l, \Bold w) = \sum_{i=1}^N \xi_i = -\sum_{i=1}^N \ln\varphi_{y_i}(\Bold x_i|\Bold a, b).
\end{split}
\end{equation}
\par
Значимым на этапе обучения является также и положительный гиперпараметр $C = \sigma^2\lambda > 0$. Этот гиперпараметр, также как и матрица $B^{-1}$ связывает параметры эвристического подхода к классификации \textrm{SVM} и байесовского критерия обучения, но в отличие от нее является глобальным параметром дисперсии в пространстве векторов параметров и признаков. В отличие от ковариационной матрицы $B^{-1}$, которая показывает степень зависимости распределений координат вектора параметров, глобальная дисперсия $C$ с уменьшением дает меньшую область возможных значений вектора параметров модели, т.е. исходя из уменьшения $\sigma$ в распределении \eqref{eq:denseW} увеличивается и норма вектора $\Bold a$. То же можно сказать и о гиперпараметре $\lambda$, который, исходя из \eqref{eq:denseXY}, при своем уменьшении  увеличивает вероятность появления объектов вне своего класса и внутри разделяющей полосы. Это однозначно соответствует ситуации, когда при уменьшении параметра $C$ в эвристическом критерии обучения \textrm{Soft-margin SVM} \eqref{eq:SMSVM}, процесс минимизации всего функционала качества $Q$ приводит к сильному уменьшению регуляризатора $\gamma(\Bold w)$ и увеличению функционала потерь.
\par
Итак, для соответствия задачи, при которой проводиться поиск наилучшей обобщающей способности при малом количестве векторов, функционал потерь, т.е. сумма ошибок разделения объектов гиперплоскостью, может быть достаточно большой, но вместо этого, регуляризация помогает уменьшить возможные отклонения от предполагаемой формы множества параметров или объектов. В пункте \ref{ssec:kernel}, рассмотрим подробнее критерий гладкости необходимый для наложения правила на вектор параметр и учет этого правила в регуляризаторе. Этим способом снижается качество распознавания на обучающей выборке и сужается область допустимых значений решающего правила, позволяя улучшить качество распознавания на генеральной совокупности.

\subsection{Двойственная задача}
\label{ssec:dual}
Рассмотрим подробнее вид оптимизационной задачи \textrm{Soft-margin SVM} \eqref{eq:SMSVM}.
Данная задача является классической задачей квадратичного программирования:
\begin{equation}
\left\{ \begin{split}
&(1/2) \Bold x^\intcl P \Bold x + \Bold q^\intcl \Bold x \rightarrow \min\limits_{x}\\
&G \Bold x \leq h\\
&A \Bold x = \bold b
\end{split} \right.
\end{equation}

Минимизируемый функционал состоит из квадратичной части по первым $d$ переменным и линейной - по следующим $N$ переменным, функции участвующие в ограничениях типа неравенств линейные, значит это также и задача выпуклого программирования, при существовании локального минимума он также будет являться глобальным минимумом. Для поиска глобального минимума необходимо выполнение условий Каруша — Куна — Таккера и условий регулярности задачи. Условия регулярности выполняются из свойств линейности ограничений и сильной двойственности (доказывается через выполнения условия Слейтора). Далее рассмотрим условия Каруша — Куна — Таккера:

Функция Лагранжа для данной задачи будет иметь вид:
\begin{equation}
L(\Bold a, b, \Bold \xi, \Bold \lambda, \Bold \mu) = \frac12 \Bold a^\intercal B \Bold a + C \sum\limits_{i=1}^N \xi_i - \sum_{i=1}^N \lambda_i \left[y_i(\Bold a^\intcl \Bold x_i + b)- 1 + \xi_i \right] - \sum_{i=1}^N \mu_i \xi_i
\end{equation}
, где $\lambda_i \geq 0, \mu_i \geq 0, i=\Seq N$- двойственные переменные.
По условиям ККТ градиенты по переменным прямой задачи:
\begin{equation}
\label{eq:KKT}
\left\{ \begin{split}
&\nabla_{\Bold a}L = \frac12 (B + B^\intcl) \Bold a - \sum_{i=1}^N \lambda_i y_i \Bold x_i = 0\\
&\nabla_{\Bold{\xi}}L = \Bold \lambda + \Bold \mu = \Bold 0\\
&\nabla_{b}L = \sum_{i=1}^N \lambda_i y_i = 0
\end{split}
\right.
\Leftrightarrow \quad
\left\{ \begin{split}
&\Bold a = \sum_{i=1}^N \lambda_i y_i \Bold x_i\\
&0 \leq \lambda_i \leq C,\, i=\Seq N\\
&0 \leq \mu_i \leq C,\, i=\Seq N\\
&\sum_{i=1}^N \lambda_i y_i = 0
\end{split}
\right.
\end{equation}
Существование обратной матрицы и ее симметричность доказывается в пункте \ref{ssec:kernel}.
Двойственная функция будет равняться:
\begin{equation}
\begin{split}
&W(\Bold \lambda, \Bold \mu) = \frac12 \sum_{i=1}^N \sum_{j=1}^N (y_i y_j (B^{-1}\Bold x_i)^\intcl B (B^{-1} \Bold x_j)) \lambda_i \lambda_j + C \sum_{i=1}^N \xi_i -\\ &-\sum_{i=1}^N \sum_{j=1}^N (y_i y_j (B^{-1} \Bold x_i)^\intcl \Bold x_j)\lambda_i \lambda_j - \sum_{i=1}^N \lambda_i y_i b + \sum_{i=1}^N \lambda_i = {условия \eqref{eq:KKT}} =\\ & = \sum_{i=1}^N \lambda_i - \frac12 \sum_{i=1}^N \sum_{j=1}^N (y_i y_j \Bold x_i^\intcl B^{-1}\Bold x_j) \lambda_i \lambda_j
\end{split}
\end{equation}
И двойственную задачу к прямой, зависящую только от $\lambda$, мы можем записать в виде:
\begin{equation}
\label{eq:dual}
\left\{\begin{split}
&W(\Bold \lambda) = \frac12 \sum_{i=1}^N \sum_{j=1}^N (y_i y_j \Bold x_i^\intcl B^{-1}\Bold x_j) \lambda_i \lambda_j - \sum_{i=1}^N \lambda_i \rightarrow \min\limits_{\lambda}\\
&\sum_{i=1}^N \lambda_i y_i = 0;\\
&0 \leq \lambda_i \leq C,\, i=\Seq N.
\end{split}
\right.
\end{equation}
Соответствующий ему вектор параметров $\Bold a$ и классификатор зависят от двойственной переменной $\lambda$: 
\begin{align}
&\Bold a =\sum_{i=1}^N \lambda_i y_i (B^{-1} \Bold x_i) \label{eq:a};\\
&a(\Bold x) = \sign\left(\sum_{i=1}^N \lambda_i y_i \Bold x_i^\intcl B^{-1} \Bold x_j + b\right)\label{eq:a(x)}.
\end{align}

Из условий дополняющей нежесткости ККТ:
\begin{equation*}
\left\{
\begin{split}
&\lambda_i \left(y_i(\Bold a^\intercal \Bold x_i + b) - 1 + \xi_i\right) = 0,\, i=\Seq N;\\
&\mu_i \xi_i = 0,\, i=\Seq N.
\end{split}\right.
\end{equation*}

Можно определить опорные вектора - объекты $\Bold x_i$, у которых соответствующие им двойственные переменные положительные. При этом, если $\lambda_i \neq C$, то соответствующие $\mu_i > 0 \Rightarrow \xi_i = 0 \Rightarrow y_i(\Bold a^\intercal \Bold x_i + b) - 1 = 0$. Тогда можно взять все вектора $x_i$ с  $0 < \lambda_i < C$, домножим на последнее равенство и получим:
\begin{align*}
&\sum\limits_{i: 0 < \lambda_i < C}\lambda_i (\Bold a^\intercal \Bold x_i + b) = \sum\limits_{i: 0 < \lambda_i < C}\lambda_i y_i; \qquad \Rightarrow \\
&b = \cfrac{-\left(\sum\limits_{i: 0 < \lambda_i < C}\lambda_i \Bold a^\intercal \Bold x_i + \sum\limits_{i: 0 < \lambda_i < C}\lambda_i y_i\right)}{\sum\limits_{i: 0 < \lambda_i < C}\lambda_i};
\qquad \Rightarrow \\
&b = \cfrac{-\sum\limits_{i: \lambda_i \neq C}\lambda_i \Bold a^\intercal \Bold x_i + C\sum\limits_{i: \lambda_i = C}y_i}{\sum\limits_{i: \lambda_i \neq C}\lambda_i}
\end{align*}

Двойственную задачу удобнее решать чем прямую по нескольким причинам. Во-первых, как уже было сказано, концепция опорных векторов была изобретена именно в этом виде и предполагала нахождение нескольких объектов из обучающей совокупности, определяющих вид и форму гиперплоскости. Во-вторых, что более существенно, критерий гладкости в данной задачи представляет собой переход в новое спрямляющее пространство. Рассмотрим свойства матрицы $B$ стоящей в прямой задаче при векторе параметров $\Bold a$, а также ее обратная матрица при векторе признаков объекта $\Bold x_i$.

\subsection{Критерий гладкости и Kernel-trick (The Method of Convolution)}
\label{ssec:kernel}
Метод \textrm{SVM} имеет свойство использовать лишь малое количество объектов выборки называемых опорными (см. пункт \ref{ssec:dual}). Этот метод хорошо работает только с дополнительными ограничений на вид разделяющей гиперплоскости, потому как опорные вектора не всегда могут показать форму границ областей различных классов на малонаполненных выборках.
Рассматривая объекты выборки $X^l$, представляющие собой результат упорядоченного измерения некоторого свойства объекта вдоль координаты той или иной природы, есть основания полагать, что соседние признаки несут почти идентичную информацию о принадлежности объекта к определенному классу. Данное свойство переносится на вид априорного распределения в пространстве векторов параметра $\Bold a = \left(a_1, a_2, \dots, a_d \right)$, которое накладывает ограничение на плавное изменение коэффициентов $a_i$ при увеличении индекса $i$ в признаковом описании объекта $\Bold x = \left(x_1, x_2, \dots, x_d \right)$. Сравнивая соседние по индексу коэффициенты, можно записать их взаимное различие в виде дополнительного регуляризатора ($\frac12$ используется для удобства):
\begin{equation}
\begin{split}
J(\Bold a) &= \frac12 \sum_{i=1}^{d-1} \left(a_i - a_{i+1}\right)^2 = \frac12 \left(a_1^2 - 2\sum_{i=1}^{d-1} a_i a_{i+1} + 2\sum_{i=1}^{d-1} a_i^2 + a_n^2 \right) = \\
&= \frac12 \sum_{i=1}^N \wtilde B_{ij} a_i a_j = \frac12 \Bold a^\intcl \wtilde B \Bold a \geq 0.
\end{split}
\end{equation}
В матричном виде матрица в критерии гладкости $\wtilde B$ принимает вид:
\begin{equation}
\wtilde B = 
\begin{pmatrix}
 1     & -1     & 0      & \cdots & \cdots & \cdots & \cdots & 0      \\
 -1    & 2      & -1     & \ddots &        &        &        & \vdots \\
 0     & -1     & 2      & -1     & \ddots &        &        & \vdots \\
\vdots & \ddots & \ddots & \ddots & \ddots & \ddots &        & \vdots \\
\vdots &        & \ddots & \ddots & \ddots & \ddots & \ddots & \vdots \\
\vdots &        &        & \ddots & -1     & 2      & -1     & 0      \\
\vdots &        &        &        & \ddots & -1     & 2      & -1     \\
 0     & \cdots & \cdots & \cdots & \cdots & 0      & -1     & 1      \\
\end{pmatrix} \in \setR^{d\times d}
\end{equation}
Очевидно, что такая постановка может привести к бесконечному множеству вырожденных решений: когда $a_i = a,\, i=\Seq N$, т.е. не различающиеся коэффициенты, $J(\Bold a) = 0$.

Поэтому критерий гладкости необходимо использовать вместе с квадратичным функционалом классической постановки SVM \eqref{eq:Q}, тогда результирующий регуляризатор в функционале качества будет иметь вид:
\begin{equation}
\gamma(\Bold w) = \frac12 \Bold a^\intcl B \Bold a = \frac12 \Bold a^\intcl \left(I + \alpha \wtilde B \right)
\end{equation}
, где $\alpha \geq 0$ будем считать параметром регуляризации, который при увеличении одновременно уменьшает критерий гладкости как слагаемое функционала. Данный вид регуляризатора $\gamma(\Bold w)$ используется в прямом виде постановки \textrm{Soft-margin SVM} \eqref{eq:SMSVM}.
\\
\\
\\
Таким образом окончательная матрица гладкости, используемая в прямой задаче \eqref{eq:SMSVM}:
\begin{equation}
B = I + \alpha \wtilde B > 0
\end{equation}
, являющейся положительно определенной матрице: для любого ненулевого вектора $\Bold v \neq 0, \Bold v^\intcl (I + \alpha \wtilde B) \Bold v = \Bold v^\intcl \Bold v \dots > 0$. Также, данная матрица является симметрической. Обращаясь к интерпретации матрицы $B$ в пункте \ref{ssec:SMSVM} будучи ковариационной матрицей многомерного случайного вектора она должна быть симметричной и неотрицательно определенной.

Из того, что матрица $B$ положительно определенная и симметричная, можно найти ортонормированный базис из ее собственных векторов, соответствующих положительным собственным значениям: $B = Q^\intcl \Lambda Q$, где $Q$ - матрица состоящая из ортонормированных собственных векторов, а $\Lambda$ - диагональная матрица с положительными собственными значениями на диагонали. При этом обратная к данной матрице будет являться матрица вида $B^{-1} = Q^\intcl \Lambda^{-1}Q$, причем она будет также положительно определенной и симметричной: $det\left(B^{-1}\right) = \left(det{B}\right)^{-1} > 0; (B^{-1})^\intcl = (B^\intcl)^{-1} = B^{-1}$. Помимо обратной матрицы рассмотрим ее квадратный корень. Из единственности разложения по ортонормированному базису $B^{-1/2} = (B^{-1}){1/2}= Q^\intcl \Lambda^{-1/2} Q$, отсюда видно, что матрица $B^{-1/2}$ также будет симметричной.

Приведем пример вида матрицы ковариации $B^{-1}$. Меняя параметр гладкости $\alpha$, можно получить разную степень зависимости между элементами. 

При $\alpha = 1$ (количество измерений $d = 10$):

\[\left[\begin{matrix}0.618 & 0.236 & 0.0902 & 0.0344 & 0.0132 & 0.00503 & 0.00192 & 0.000739 & 0.000296 & 0.000148\\0.236 & 0.472 & 0.18 & 0.0689 & 0.0263 & 0.0101 & 0.00384 & 0.00148 & 0.000591 & 0.000296\\0.0902 & 0.18 & 0.451 & 0.172 & 0.0658 & 0.0251 & 0.00961 & 0.0037 & 0.00148 & 0.000739\\0.0344 & 0.0689 & 0.172 & 0.448 & 0.171 & 0.0653 & 0.025 & 0.00961 & 0.00384 & 0.00192\\0.0132 & 0.0263 & 0.0658 & 0.171 & 0.447 & 0.171 & 0.0653 & 0.0251 & 0.0101 & 0.00503\\0.00503 & 0.0101 & 0.0251 & 0.0653 & 0.171 & 0.447 & 0.171 & 0.0658 & 0.0263 & 0.0132\\0.00192 & 0.00384 & 0.00961 & 0.025 & 0.0653 & 0.171 & 0.448 & 0.172 & 0.0689 & 0.0344\\0.000739 & 0.00148 & 0.0037 & 0.00961 & 0.0251 & 0.0658 & 0.172 & 0.451 & 0.18 & 0.0902\\0.000296 & 0.000591 & 0.00148 & 0.00384 & 0.0101 & 0.0263 & 0.0689 & 0.18 & 0.472 & 0.236\\0.000148 & 0.000296 & 0.000739 & 0.00192 & 0.00503 & 0.0132 & 0.0344 & 0.0902 & 0.236 & 0.618\end{matrix}\right]
\]

Здесь наблюдается довольно большая дисперсия у каждой компоненты вектора параметров модели $\Bold a$, хотя ее значения и меньше единицы, что обусловлено функционалом, уменьшающим норму $\Bold a$ в решающем правиле. В тоже время происходит быстрое уменьшение ковариации $B^{-1}_{ij} = cov(a_i, a_j)$ между двумя компонентами вектора. Это обусловлено тем фактом, что при данном выборе гиперпараметра $\alpha$ мы не сильно сглаживаем вектор.

При $\alpha = 100$:

\[\left[\begin{matrix}0.126 & 0.118 & 0.11 & 0.104 & 0.0983 & 0.0939 & 0.0904 & 0.0878 & 0.0861 & 0.0853\\0.118 & 0.119 & 0.111 & 0.105 & 0.0993 & 0.0948 & 0.0913 & 0.0887 & 0.087 & 0.0861\\0.11 & 0.111 & 0.114 & 0.107 & 0.101 & 0.0967 & 0.0931 & 0.0905 & 0.0887 & 0.0878\\0.104 & 0.105 & 0.107 & 0.11 & 0.104 & 0.0996 & 0.0959 & 0.0931 & 0.0913 & 0.0904\\0.0983 & 0.0993 & 0.101 & 0.104 & 0.108 & 0.103 & 0.0996 & 0.0967 & 0.0948 & 0.0939\\0.0939 & 0.0948 & 0.0967 & 0.0996 & 0.103 & 0.108 & 0.104 & 0.101 & 0.0993 & 0.0983\\0.0904 & 0.0913 & 0.0931 & 0.0959 & 0.0996 & 0.104 & 0.11 & 0.107 & 0.105 & 0.104\\0.0878 & 0.0887 & 0.0905 & 0.0931 & 0.0967 & 0.101 & 0.107 & 0.114 & 0.111 & 0.11\\0.0861 & 0.087 & 0.0887 & 0.0913 & 0.0948 & 0.0993 & 0.105 & 0.111 & 0.119 & 0.118\\0.0853 & 0.0861 & 0.0878 & 0.0904 & 0.0939 & 0.0983 & 0.104 & 0.11 & 0.118 & 0.126\end{matrix}\right]
\]

Как видно, дисперсия каждой компоненты значительно уменьшилась, для того, чтобы она и соседние с ней компоненты не сильно отличались как от нулевого вектора, так и друг от друга. Увеличение степени гладкости наблюдается при рассмотрении ковариации "дальних" компонент вектора - для $a_i$ компоненты ее дисперсия и ковариация с дургими компонентами незначительно отличаются при $\alpha = 100$, чего не скажешь при меньшем значении параметра гладкости $\alpha$. То есть, близость позиций координат вектора становится менее значимой, чем общая сглаженность их значений.

Интересно показать, как меняется форма множества векторов $\Bold a$ в пространстве (к примеру при $d = 10$). Сгенерируем с помощью многомерного нормального распределения с нулевым вектором матожидания и матрице ковариации $B^{-1}$ и возьмем проекцию на две из $d$ координат. Все описанные зависимости отражены в пространстве координат вектора $\Bold a$ на рис. \ref{fig:vis_twice10_1} и \ref{fig:vis_twice10_100}.


\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{pic/vis_twice10_alpha1.png}
\caption[]{\label{fig:vis_twice10_1} Проекции пространства векторов $\Bold a$ на две из 10 координат, при $\alpha = 1$}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{pic/vis_twice10_alpha100.png}
\caption[]{\label{fig:vis_twice10_100}  Проекции пространства векторов $\Bold a$ на две из 10 координат, при $\alpha = 100$}
\end{figure}


В работе Вапника\,В. \cite{VapnikSVM} описывается подход называемый \textrm{The Method of Convolution}  \cite{VapnikSVM}, или по-другому - ядровой переход.
Рассматривается переход из исходного пространства в спрямляющее, в каждом определена функция скалярного произведения. Спрямляющее пространство используется для удобства классификации, так как двойственная задача \eqref{eq:dual} зависит от скалярных произведений признаков объектов и таким образом линейная классификация может проводиться в спрямляющем пространстве, в то время как в исходном она имеет нелинейный вид. Таким образом найдем вид $\phi: \setR^d \rightarrow \setR^d$, такой что использованная нами матрица гладкости $B$ задействована в скалярном произведении: 
\begin{equation}
\begin{split}
&\Bold x_i^\intcl B^{-1} \Bold x_j = (B^{-1} \Bold x_j \cdot \Bold x_i) = ((B^{-1/2} \Bold x_j \cdot (B^{-1/2} \Bold x_i)) = \\
& = (\phi(\Bold x_j) \cdot \phi(\Bold x_i)) \Rightarrow
\phi(\Bold x_i) = (B^{-1/2} \Bold x_i).
\end{split}
\end{equation} 

Из единственности разложения по ортонормированному базису верно: 
\[B^{-1/2} = Q^\intcl \Lambda^{-1/2} Q\]
Для нее будут верны все свойства, что и для матрицы $B^{-1}$ - симметричность, положительная определенность.

На рисунке \ref{fig:model} показаны модельные данные, сгенерированные в пункте  \ref{sec:expmodel}, в своем исходном пространстве гладких синусоид размерности $d = 50$. Описание каждого класса синусоид проводиться по различным частотам $\omega$ и сдвигам $\phi$. Из-за небольшого различия этих гиперпараметров, классы объектов тоже не сильно отличаются. Добавив Гауссов шум  для каждого из $d$ измерений мы еще сильнее сглаживаем различия между классами.

Тем не менее в спрямляющем пространстве, определяемом матрицей гладкости $B$ (на рис. \ref{fig:H_model}) значения зашумленных синусоид сильно сглаживается, одновременно приближаясь к средним значениям в новом-спрямляющем пространстве. Стоит отметить, что и средние значения синусоид были сильно изменены в новом-спрямляющем пространстве, но из-за сближения средних значений и зашумленных легче определить принадлежность нового объекта к тому или иному классу.

Суммируя сказанное выше, оказывается, что наша матрица гладкости, участвующая в априорной плотности распределения \eqref{eq:denseW}, определяется матрице $B^{-1}$, которая положительно определенная, симметричная и при увеличении параметра. Но она учитывает зависимости всех компонент признака параметра $\Bold a$: если сравнить диагональные элементы с вне диагональными, удаляясь от диагонали все дальше, наблюдается заметное уменьшение значения ковариации между элементами, а значит и уменьшается зависимость между рассматриваемой компонентой вектора $\Bold a$ и остальными при удалении от исходной. Данная зависимость отражает эффект предполагаемой гладкости компонент.

К тому же, решения вопроса выбора ядра для реализации \textrm{SMSVM} сводиться к использовании ковариационной матрицы или ее квадратному корню, которые наилучшим образом показывают эффект гладкости в новом спрямляющем пространстве. Также, как было сказано в введении \ref{sec:intro}, сначала необходимо привести все объекты, являющиеся буквами алфавита, к единому признаковому пространству, что тоже подразумевает ядровой переход. Так как в данной работе не рассматривались реальные данные, здесь не приводится вид ядра и спрямляющего пространства при выравнивании \textrm{DTW}, в котором буквы будут иметь одинаковое количество измерений, описанное во введении \ref{sec:intro}. 

\section{Эксперименты на модельных данных}
\label{sec:expmodel}

\subsection{Исходные данные и~условия эксперимента}
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{pic/model_data_view.png}
\caption[]{\label{fig:model} Модельные данные в исходном пространстве}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{pic/H_model_data_view.png}
\caption[]{\label{fig:H_model} Модельные данные в спрямляющем пространстве}
\end{figure}

Для генерирования модельных данных были выбраны синусоиды с различными частотами $\omega$ и сдвигами $\phi$ так, чтобы формы кривых синусоид отличались незначительно. Также для каждого из $d = 50$ значений синусоид, накладывался шум с параметром среднего значения в исходных точках синусоид, чтобы возможные различия между объектами классов еще сильнее уменьшались. Несмотря на это предполагалось, что по средним значениям координат различных объектов, можно оценить принадлежность к тому или иному классу, но для этого было необходимо применить критерий гладкости, усредняющий и приближающий зашумленные вектора к своим средним значениям. На рис. \ref{fig:model} и \ref{fig:H_model} наглядно показан вид этих данных как в исходном, так и в спрямляющем пространстве ядрового перехода с матрицей $B^{-1/2}$.

Далее проводилась попарная классификация всех классов. Для этого настраивался \textrm{SMSVM} с ядром на подвыборках объектов различных классов. Из предположения о малой наполненности исходной выборки, и сравнимого с размерность признакового пространства данных $d$ количества объектов $N$ в классифицируемой выборке, выбирались случайные подвыборки объектов двух классов (равномощные для обоих классов) размерами $N = 25, 50, 100, 150, 300$ для контроля и столько же для обучения.

Гиперпараметр $C = 0.1$ брался для того, чтобы оставалась возможность достаточно больших значений ошибок классификации на обучении $\sum_{i=1}^N \xi_i$, но критерий гладкости и критерий разделения разделяющей полосы имели больший эффект. Точность определялась как отношение правильно классифицируемых объектов контрольной выборки к их общему количеству. Для уменьшения случайности выбора подвыборок бралось усреднение точностей, полученных на 50 различных разделений выборки на контроль и обучение. 

Проводилось исследование оптимальных значений параметра гладкости $\alpha$ для разного соотношения между количеством объектов в выборке и размерности признакового пространства. При $\alpha = 0$ отключался критерий гладкости в применяемой модели линейной классификации.  Точности всех бинарных классификаций объектов усреднялись для получения среднего показателя точность для одного и того же параметра гладкости $\alpha$.

\subsection{Результаты эксперимента}
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{pic/score.png}
\caption[]{\label{fig:score} Усредненная точность бинарной классификации в $d = 50$ -мерном пространстве признаков при увеличении параметра гладкости $\alpha$}.
\end{figure}


По результатам приведенным на рис. \ref{fig:score} хорошо видно, что даже при незначительном увеличении $\alpha = 5$ наблюдается возрастание точности классификации. Но при очень больших значениях $\alpha$ происходит эффект переобучения, так как критерий гладкости начинает сильно ограничивать вид нашего пространства векторов параметров $\Bold a$. 

Сравнивая показатели точности, для разных соотношений между количеством объектов в обучающей выборке и признаковым пространством, хорошо заметны некоторые зависимости. Так, критерий гладкости дает наиболее значительный прирост в точности для малонаполненных выборок, а при оптимальном подборе $\alpha$ в результате получается точность даже выше, чем при классификации на достаточно больших обучающих выборках, но без учета критерия гладкости. С другой стороны, момент переобучения достигается раньше на таких малонаполненных выборках, чего можно избежать применением кросс-валидации по параметру $\alpha$. 

\subsection{Выводы}
В общем, для всех размеров подвыборок оптимальными $\alpha$ будут значения в отрезке $[80:125]$, но, данный показатель может варьироваться исходя из вида, свойства и формы рассматриваемых объектов-сигналов. В случае выборки сравнимой по количеству объектов со своим признаковым пространством ($d = 50, N = 50$), вместе со значительными приростом точности в сравнении с нулевым параметром гладкости наблюдаются высокие показатели близкие к показателям на более больших выборках, т.е. критерий гладкости становится менее значимым при увеличении количества объектов.

\section{Заключение}

Критерий гладкости описанный в данной работе имеет схожую вероятностную интерпретацию с эвристическим подходом. Рассмотренный вид объектов-параметров модели классификации в пространстве, ограниченном свойствами гладкости дал вероятностную интерпретацию зависимости значений его компонент друг от друга, а также изменение этой зависимости при варьировании степени гладкости решающего правила.

Проведенные эксперименты на модельных данных дали численные результаты, при которых показана значимость применения критерия гладкости для задач с малым количеством объектов в обучении. 

Рассмотренный вид регуляризации решающего правила дал один из способов использования априорной информации в задачах анализа данных.

В последующих работах предполагается осветить вопросы выравнивания признакового описания сигналов и использования его для работы с реальными данными.

\newpage

\renewcommand{\bibname}{Список литературы}
\addcontentsline{toc}{section}{\bibname}

%\nocite{hastie09elements,bishop06pattern,zhuravlev06recognition,zhuravlev78prob33,shlezinger04ten,boucheron05theory}

\def\BibUrl#1.{}\def\BibAnnote#1.{}
%\def\BibUrl#1{\\{\footnotesize\tt\def~{\char126} http://#1}}
\bibliographystyle{gost71s}
\bibliography{MachLearn}

\begin{thebibliography}{0}

\bibitem{Bahlman}
\emph{Bahlmann\,C.\,} On-line Handwriting Recognition with Support Vector Machines — A Kernel Approach --- 2003.

\bibitem{DTW}
\emph{Senin\,P.\,} Dynamic Time Warping Algorithm Review --- 2008.

\bibitem{VapnikSVM}
\emph{Vapnik\,V. and Cortes\,C.\,} Support-Vector Networks --- 1995.
%\bibitem{}
%Ching-Pei Lee, Chih-Jen Lin,
\bibitem{MLP}
\emph{M.\,J. Castro-Bleda, D.\,Llorens\,} Improving a DTW-based Recognition Engine for On-line Handwritten Characters by Using MLPs --- 2009
\bibitem{HMM}
\emph{B.\,Q.\,Huang, C.\,J.\,Du\,} A Hybrid HMM-SVM Method for Online Handwriting Symbol Recognition --- 2006

\bibitem{Krasotkina}
\emph{Красоткина О.\,В.\,} Методы учета априорной информации в задачах распознавания образов --- 2000.

\bibitem{tatar09bayes}
\emph{Татарчук\,А.\,И., Сулимова\,В.\,В., Моттль\,В.\,В., Уиндридж\,Д.\,} Метод
релевантных потенциальных функций для селективного комбинирования
разнородной информации при обучении распознаванию образов на основе
байесовского подхода.
--- Всероссийская конференция ММРО-14.М. , МАКС Пресс, 2009.C. 188–191.
%\emph{L2\_Hinge Loss in MultiCl SVM}
%\url{http://ntu.csie.org/~cjlin/papers/l2mcsvm/l2mcsvm.pdf}
%\bibitem{}
%\LaTeX\, wiki
%\url{http://en.wikibooks.org/wiki/LaTeX}
\end{thebibliography}

\end{document}
